# Web Scraping Job Requirements Analysis

## ðŸ“Œ Project Overview
This project automates the process of collecting job postings for Data Analyst roles from an online job portal using **Selenium**. The scraped data is analyzed to identify hiring trends, in-demand skills, top employers, and location-based opportunities.

## ðŸŽ¯ Objectives
- Scrape Data Analyst job postings from multiple pages.
- Extract relevant information such as job title, company, location, salary, and skills.
- Perform exploratory data analysis (EDA) to identify trends.
- Present insights in an easy-to-read format for recruitment purposes.

## ðŸ›  Tools & Libraries
- **Python**
- **Selenium** (web scraping)
- **Pandas** (data processing)
- **Matplotlib** (visualizations)

## ðŸ“Š Key Insights
- **Top Hiring Companies:** [Your top 5 company names]
- **Most Common Job Locations:** [Top 5 locations]
- **Most In-demand Skills:** [List of top skills]
- Salary trends: [If data available]

## ðŸ“‚ Files in This Repository
- `web_scraping_job_requirements.ipynb` â†’ Full scraping + analysis code.
- `job_postings.csv` â†’ Raw scraped data.
- `requirements.txt` â†’ Python dependencies.

## ðŸš€ How to Run
1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/Web_Scraping_Job_Requirements.git
   cd Web_Scraping_Job_Requirements
   ```
2. Install dependencies:
```
pip install -r requirements.txt
```
3. Run the notebook in Jupyter:
```
jupyter notebook web_scraping_job_requirements.ipynb
```
ðŸ‘¤ Author
Nitesh â€” Aspiring Applied AI Scientist

